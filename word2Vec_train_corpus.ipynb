{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nlp as nlp_hlp\n",
    "import helpers_and_variables as hlp\n",
    "import ML_helpers as ml_hlp\n",
    "import shap\n",
    "import random\n",
    "# import pixiedust\n",
    "# %%pixie_debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read the data\n",
    "\"\"\"\n",
    "# labels_path = \"\"\n",
    "# dict_path = \"\"\n",
    "labels = hlp.load_list_from_json_file()\n",
    "main_dict = hlp.load_dict_from_json_file()\n",
    "# print(len(labels), len(main_dict))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this was token from load and run pretrained, used to train the word2vec model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get external swedish corpus, swedish news\n",
    "\"\"\"\n",
    "file_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/saved_stuff/corpus/corpus_swedishNews_tokenized\"\n",
    "corpus_swedishNews_tokenized = hlp.load_list_from_json_file(file_path=file_path)\n",
    "stemmed_swedishNews_corpus = nlp_hlp.get_stemmed_corpus(corpus_swedishNews_tokenized)\n",
    "del corpus_swedishNews_tokenized\n",
    "\"\"\"\n",
    "Get external swedish press\n",
    "\"\"\"\n",
    "file_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/saved_stuff/corpus/Data/covid-210201_1\"\n",
    "corpus_press_conferans = hlp.load_list_from_json_file(file_path=file_path)\n",
    "corpus = list()\n",
    "for elem in corpus_press_conferans:\n",
    "    text = elem['Transkribering']\n",
    "    tmp = list()\n",
    "    for word in text.split():\n",
    "        tmp.append(word)\n",
    "    corpus.append(tmp)\n",
    "corpus_press_conferans = corpus\n",
    "del corpus\n",
    "stemmed_press_conferans_corpus = nlp_hlp.get_stemmed_corpus(corpus_press_conferans)\n",
    "del corpus_press_conferans\n",
    "from xml.dom import minidom\n",
    "file_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/saved_stuff/corpus/aspacsven-sv.xml\"\n",
    "# parse an xml file by name\n",
    "file = minidom.parse(file_path)\n",
    "sentences = file.getElementsByTagName('sentence')\n",
    "corpus_svenska = list()\n",
    "for elem in sentences:\n",
    "    words = elem.getElementsByTagName('w')\n",
    "    tmp = list()\n",
    "    for w in words:\n",
    "        tmp.append(w.firstChild.data)\n",
    "        # print(w.firstChild.data)\n",
    "    corpus_svenska.append(tmp)\n",
    "stemmed_svenska_corpus = nlp_hlp.get_stemmed_corpus(corpus_svenska)\n",
    "del corpus_svenska, minidom\n",
    "from xml.dom import minidom\n",
    "file_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/saved_stuff/corpus/aspacsven-en.xml\"\n",
    "\n",
    "# parse an xml file by name\n",
    "file = minidom.parse(file_path)\n",
    "sentences = file.getElementsByTagName('sentence')\n",
    "corpus_english = list()\n",
    "\n",
    "for elem in sentences:\n",
    "    words = elem.getElementsByTagName('w')\n",
    "    tmp = list()\n",
    "    for w in words:\n",
    "        tmp.append(w.firstChild.data)\n",
    "    corpus_english.append(tmp)\n",
    "stemmed_english_corpus = nlp_hlp.get_stemmed_corpus(corpus_english, stemm=False, stemm_by_nltk=True, nltk_lang='english')\n",
    "\n",
    "del minidom, corpus_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenat_corpus = stemmed_own_corpus + stemmed_english_corpus + stemmed_swedishNews_corpus  + stemmed_press_conferans_corpus + stemmed_svenska_corpus \n",
    "# concatenat_corpus_words = hlp.get_array_of_words_from_list_of_lists_of_sentences(concatenat_corpus)\n",
    "# swe_news_corpus_words = hlp.get_array_of_words_from_list_of_lists_of_sentences(stemmed_swedishNews_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gnäll', 0.6499810218811035), ('tänk', 0.648897647857666), ('slipp', 0.6337395906448364), ('bry', 0.6262148022651672), ('strunt', 0.6178288459777832), ('leds', 0.6163414716720581), ('dumt', 0.6116852760314941), ('tråk', 0.6066194772720337), ('sånt', 0.6057609915733337), ('sä', 0.5979984402656555)]\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# # \"\"\"\n",
    "# # Create a word2Vec model and train it. \n",
    "# # Save the trained model and the word vectores obtained,\n",
    "# # if already exist, just load it \n",
    "# # \"\"\"\n",
    "# # # vector_size = 100\n",
    "# # # own_model = Word2Vec(concatenat_corpus, vector_size=vector_size, window=5, min_count=1, workers=4, sorted_vocab=1, shrink_windows=True, sg=1)\n",
    "# # # own_word_vectors = own_model.wv\n",
    "# file_path_model = \"C:/Users/a7mad/Desktop/MEX/PekLung/wiki_word2vec.model\"\n",
    "# # # own_model.save(file_path_model)\n",
    "# # # #Store just the words + their trained embeddings.\n",
    "# # # file_path_vectors = \"C:/Users/a7mad/Desktop/MEX/PekLung/word2vec.wordvectors\"\n",
    "\n",
    "# # # own_word_vectors.save(file_path_vectors)\n",
    "# # # del own_model, own_word_vectors\n",
    "# own_model = Word2Vec.load(file_path_model)\n",
    "# # file_path_vectors = \"C:/Users/a7mad/Desktop/MEX/PekLung/wiki_word2vec.wordvectors\"\n",
    "# # own_word_vectors = KeyedVectors.load(file_path_vectors, mmap='r')\n",
    "# own_word_vectors = own_model.wv\n",
    "print(own_word_vectors.most_similar('ork'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = concatenat_corpus\n",
    "wv = own_word_vectors\n",
    "# test = np.in1d(sorted_words, wv.index_to_key[0:40000], assume_unique=True)\n",
    "# print(sorted_words[np.where(test == False)])\n",
    "# print(test)\n",
    "len(sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#  code is trying to \"divide by zero\" or \"divide by NaN\". \n",
    "# If you are aware of that and don't want it to bother\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "j =0\n",
    "labels= list()\n",
    "# tokens = list()\n",
    "labels.append(unique_corpos[0][0])\n",
    "# tokens.append(wv[unique_corpos[0][0]])\n",
    "\n",
    "tokens = np.empty((100,))\n",
    "tokens[0:] = wv[unique_corpos[0][0]]\n",
    "tokens = np.reshape(tokens, (100, 1))\n",
    "\n",
    "\"Create TSNE model and plots it\"\n",
    "j=0\n",
    "# for word in model1.wv.vocab:\n",
    "# for sen in corpus:\n",
    "\n",
    "# for word in unique_corpos[0][500:]:\n",
    "for word in wv.index_to_key[0:100]:\n",
    "    try:\n",
    "        tokens = np.hstack((tokens, wv[word].reshape((100,1))))\n",
    "        # labels.append(word)\n",
    "        tsne_model = TSNE(perplexity=42, n_components=2, learning_rate='auto', n_jobs=-1, min_grad_norm=1e-3,\n",
    "                            init='pca', n_iter=250, n_iter_without_progress=100, random_state=23)\n",
    "\n",
    "        new_values = tsne_model.fit_transform(tokens)\n",
    "        # print(new_values)\n",
    "        x = []\n",
    "        y = []\n",
    "        # except:\n",
    "            # continue\n",
    "        if word in sorted_words:\n",
    "            labels.append(word)\n",
    "\n",
    "            for value in new_values:\n",
    "                x.append(value[0])\n",
    "                y.append(value[1]) \n",
    "            j = j+1\n",
    "    except:\n",
    "        pass\n",
    "    # there is too many words and my computer doesnot like to run this so i have a break below\n",
    "    # if j==200:\n",
    "    #     break\n",
    "print(j)\n",
    "# umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "\n",
    "fig_obj = plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "    xy=(x[i], y[i]),\n",
    "    xytext=(5, 2),\n",
    "    textcoords='offset points',\n",
    "    ha='right',\n",
    "    va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "    xy=(x[i], y[i]),\n",
    "    xytext=(5, 2),\n",
    "    textcoords='offset points',\n",
    "    ha='right',\n",
    "    va='bottom')\n",
    "plt.show()\n",
    "# plt.plot(x, y)\n",
    "with open('myplot.pickle','w') as fid:\n",
    "    pickle.dump(ax, fid)\n",
    "# pickle.dump(fig_obj, file('myplot.pickle', 'w'))\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "with open('myplot.pickle','rb') as fid:\n",
    "    ax = pickle.load(fid)\n",
    "plt.show()\n",
    "ax.axes.axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## untill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert dict to data list and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import re\n",
    "# import nltk\n",
    "# from gensim.models import word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get a cleaned data list (stemming and removing stop words), with text for each patient.\n",
    "list of lists of sentences with tokenized words building a corpus for word2Vec, \n",
    "return_corpus_token=True and return_corpus_sent=False,\n",
    "\n",
    "corpus_sentenses_tokenized would be in the format:\n",
    "[[\" \", \" \"], [\" \", \" \", \" \"], [...]...]\n",
    "\n",
    "\"\"\"\n",
    "data_list, corpus_sentenses, corpus_sentenses_tokenized = nlp_hlp.get_data_list_from_main_dict(main_dict, stemm=True, \n",
    "return_corpus_sent=False, return_corpus_token=True)\n",
    "del corpus_sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a word2Vec model and train it. \n",
    "Save the trained model and the word vectores obtained,\n",
    "if already exist, just load it \n",
    "\"\"\"\n",
    "# vector_size = 10\n",
    "# model = word2vec.Word2Vec(corpus_sentenses_tokenized, size=vector_size, window=20, min_count=2, workers=4)\n",
    "# word_vectors = model.wv\n",
    "# file_path_model = \"C:/Users/a7mad/Desktop/MEX/PekLung/word2vec.model\"\n",
    "# model.save(file_path_model)\n",
    "# #Store just the words + their trained embeddings.\n",
    "file_path_vectors = \"C:/Users/a7mad/Desktop/MEX/PekLung/word2vec.wordvectors\"\n",
    "# word_vectors.save(file_path_vectors)\n",
    "# del model, word_vectors\n",
    "\n",
    "#model = Word2Vec.load(file_path_model)\n",
    "wv = KeyedVectors.load(file_path_vectors, mmap='r')\n",
    "# wv['rök']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW or Skip gram architecture, for word2Vec\n",
    "# model1 = gensim.models.Word2Vec(corpus_sentenses_tokenized, min_count = 1,size = vector_size, window = 5, sg=1) \n",
    "# model2 = gensim.models.Word2Vec(corpus_sentenses_tokenized, min_count = 1, size = vector_size, window = 5, sg = 1)\n",
    "# model1.wv['rökt'] == model2.wv['rökt']\n",
    "# del model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the features and see similarity in distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "%matplotlib qt \n",
    "#  code is trying to \"divide by zero\" or \"divide by NaN\". \n",
    "# If you are aware of that and don't want it to bother\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "j =0\n",
    "labels= list()\n",
    "tokens = list()\n",
    "# for i in model1.wv.vocab:\n",
    "for i in wv.vocab:\n",
    "    token = i\n",
    "    labels.append(i)\n",
    "#     print(token)\n",
    "    j=j+1\n",
    "    if j==2:\n",
    "        break\n",
    "    tokens.append(wv[token])\n",
    "print(np.shape(tokens))\n",
    "\n",
    "\n",
    "\"Create TSNE model and plots it\"\n",
    "j=0\n",
    "# for word in model1.wv.vocab:\n",
    "for word in wv.vocab:\n",
    "    tokens.append(wv[word])\n",
    "    labels.append(word)\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, \n",
    "                      init='pca', n_iter=250, random_state=23)\n",
    "    \n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])  \n",
    "    j = j+1\n",
    "    # there is too many words and my computer doesnot like to run this so i have a break below\n",
    "    if j==250:\n",
    "        break\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "    xy=(x[i], y[i]),\n",
    "    xytext=(5, 2),\n",
    "    textcoords='offset points',\n",
    "    ha='right',\n",
    "    va='bottom')\n",
    "plt.show()\n",
    "# umap\n",
    "# %matplotlib inline "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
