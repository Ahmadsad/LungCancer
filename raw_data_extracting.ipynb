{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers_and_variables as hlp\n",
    "import ML_helpers as ml_hlp\n",
    "import shap\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "# import pixiedust\n",
    "# %%pixie_debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read the row data as pd data frame\n",
    "\"\"\"\n",
    "dataPath = \"C:/Users/a7mad/Desktop/MEX/PekLung/PekLUng_20210503sav.xlsx\"\n",
    "password =  \"d46cf574-84e1-11ec-a8a3-0242ac120002\"#\n",
    "rawdataDF = hlp.get_exL_df(stringPath=dataPath, password=password, sheetNum=1)\n",
    "\"\"\" \n",
    "read the data info as pd data frame\n",
    "\"\"\"\n",
    "dataInfoPath = \"C:/Users/a7mad/Desktop/MEX/PekLung/Datainformation_minroAdjusted.xlsx\"\n",
    "dataInfoDF = hlp.get_cleaned_dataInfo_df(dataInfoPath)\n",
    "katInfoDF = hlp.get_cleaned_katInfo_df(dataInfoPath)\n",
    "\"\"\"\n",
    "get dictionary of data information from data info data fram\n",
    "\"\"\"\n",
    "dict_of_katInfo = hlp.get_dict_of_katInfoDF(katInfoDF)\n",
    "dict_of_dataInfo = hlp.get_dict_of_dataInfoDF(dataInfoDF)\n",
    "\"\"\" \n",
    "Get the labels, under name Lungcancer_Num\n",
    "check whether labels are 1=yes LC or 2=No LC and check STUDY_1 if valid, remove unlabeled and invalid patients\n",
    "\n",
    "\"\"\"\n",
    "rawdataDF, labels, removed_indices = hlp.get_labels_and_indices_unlabeled_patients(rawdataDF)\n",
    "\"\"\"\n",
    "Remove features, which includes information about the label, like diagnos2, aslo modules names\n",
    "DiagnosticInvestigation (need to be discussed, since it includes dignostic which means non early prediction).\n",
    "columns_tobe_removed=None --> predefind columns will be removed, see the function in helpers_and_variables file.\n",
    "remove_cols_with_dates=True --> removes all columns with dates(this is relevance in case of tfidf), \n",
    "otherwise consider using converting dates to days, see function hlp.get_dates_in_days() in next cell.\n",
    "\"\"\"\n",
    "rawdataDF =  hlp.get_dataframe_without_cols(rawdataDF,  columns_tobe_removed=None, remove_cols_with_dates=False)\n",
    "\n",
    "copy_rawdata = pd.DataFrame.copy(rawdataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_rawdata = hlp.get_dates_in_days(copy_rawdata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     display(katInfoDF)\n",
    "# copy_rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_vars = katInfoDF['Variable'].tolist()\n",
    "# list_of_labels = katInfoDF['Label'].tolist()\n",
    "# list_of_values = katInfoDF['Value'].tolist()\n",
    "\n",
    "# list_of_questions = dataInfoDF['Label'].tolist()\n",
    "# list_of_quesNams = dataInfoDF['Variable'].tolist()\n",
    "\n",
    "# cleanedListOfStrings = hlp.get_cleaned_list_of_strings(list_of_questions, stemm=True)\n",
    "# cleanedListOfStrings\n",
    "# cleanedWordsArray = hlp.get_array_of_words(cleanedListOfStrings)\n",
    "\"\"\"\n",
    "get a list with qustions and answers for one patient\n",
    "\"\"\";\n",
    "# list_ques_answers = list(main_dict['1001'].values())\n",
    "# list_cleaned_ques_answers = hlp.get_cleaned_list_of_strings(list_ques_answers,stemm=False)\n",
    "# token_words_array = hlp.get_array_of_words(list_cleaned_ques_answers)\n",
    "# unique_token_words = np.unique(token_words_array, return_counts=True)\n",
    "# stemmed_by_nltk = hlp.get_stemmed_strings_as_nltk_SnowballStemmer(list_of_questions, ignore_stopwords=False)\n",
    "# tokenized_by_nltk =  hlp.get_tokenized_strings_by_nltk(cleanedListOfStrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_with_dates = hlp.get_cols_with_dates(copy_rawdata)\n",
    "# df_dates_before = pd.DataFrame(columns=cols_with_dates)\n",
    "# df_dates_after = pd.DataFrame(columns=cols_with_dates)\n",
    "# for column in cols_with_dates:\n",
    "#     df_dates_before[column] = rawdataDF[column]\n",
    "#     df_dates_after[column] = copy_rawdata[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_rawdata.to_csv('C:/Users/a7mad/Desktop/MEX/PekLung/dates_adjusted.csv')\n",
    "# copy_rawdata.to_excel('C:/Users/a7mad/Desktop/MEX/PekLung/dates_adjusted_excel.xlsx')\n",
    "\n",
    "# x = pd.read_csv('dates_adjusted.csv')\n",
    "# x = pd.read_excel('dates_adjusted_excel.xlsx')\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     display(df_dates_before)\n",
    "# display(df_dates_before)\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):    \n",
    "#     display(df_dates_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most number of features removed from a one patient is:  566\n",
      "Patient index with most removed features is:  10\n",
      "Patient number or id: 1013, with remaining number of features: 113\n",
      "\n",
      "Total length of the main dictionary, number of patients:  506\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "get a dictionary of each patient according to this format:\n",
    "Patient number: {questions name, the questions text, corresponding answers}\n",
    "If clear_missings_or_Non true, it clears questions (features),\n",
    "where answeres are missing or the answers is no/missing.\n",
    "amount_data = None --> all data is considered\n",
    "\"\"\"\n",
    "main_dict, ind_num_removed_features = hlp.get_dict_of_questions_answers(copy_rawdata, \n",
    "                                                                        dataInfoDF, \n",
    "                                                                        katInfoDF, \n",
    "                                                                        amount_data = None,\n",
    "                                                                        clear_missings_or_Non=True,\n",
    "                                                                        clear_ques_with_negative_answeres=False)\n",
    "\"\"\"\n",
    "check how many features were removed by get_dict_of_questions_answers()\n",
    "\n",
    "\"\"\"\n",
    "max_num_feature_removed = 0\n",
    "ind_of_max_feature_removed = 0\n",
    "\n",
    "for ind, count in ind_num_removed_features:\n",
    "    if count > max_num_feature_removed:\n",
    "        max_num_feature_removed = count\n",
    "        ind_of_max_feature_removed = ind\n",
    "print(\"Most number of features removed from a one patient is: \", max_num_feature_removed)\n",
    "print(\"Patient index with most removed features is: \", ind_of_max_feature_removed)\n",
    "print(\"Patient number or id: {0}, with remaining number of features: {1}\".format(str(int(rawdataDF.loc[ind_of_max_feature_removed][0])), len(main_dict[str(int(rawdataDF.loc[ind_of_max_feature_removed][0]))])))\n",
    "print(\"\\nTotal length of the main dictionary, number of patients: \", len(main_dict))\n",
    "#main_dict[str(int(rawdataDF.loc[ind_of_max_feature_removed][0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In case you need to write/save the dictionary\n",
    "\"\"\"\n",
    "save_dict_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/dict\"\n",
    "# save_dict_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/dict_nodates\"\n",
    "# save_dict_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/dict_removed_nos_nodates\"\n",
    "# labels_path = \"C:/Users/a7mad/Desktop/MEX/PekLung/labels\"\n",
    "\n",
    "hlp.write_dict_as_json_file(main_dict, file_path=save_dict_path)\n",
    "# hlp.write_list_as_json_file(labels, file_path=labels_path)\n",
    "js = hlp.load_dict_from_json_file(file_path=save_dict_path)\n",
    "# lbl = hlp.load_list_from_json_file(file_path=labels_path)\n",
    "# print(lbl == labels)\n",
    "print(js==main_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = hlp.get_data_list_from_main_dict(main_dict, stemm=True)\n",
    "# data_list\n",
    "# vectorizer = ml_hlp.get_tfidf_vectorization_model(data_list)\n",
    "# X_train_vectorized = ml_hlp.get_tfidf_vectorized_data(tfidf_model=vectorizer,\n",
    "#                                                       to_be_vectorized_data=data_list)\n",
    "# X_train = ml_hlp.get_csr_matrix(X_train_vectorized)\n",
    "# main_dict['1001']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
